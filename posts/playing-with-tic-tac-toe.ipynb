{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DzpFpoujKqON"
   },
   "source": [
    "## Tic Tac Toe\n",
    "### Goals\n",
    "With this, my main goal was to mess around with both using straight numpy to build a basic tic tac toe agent as well as to learn more about qlearning as well. [This article](https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/) helped me with getting started with the basics.\n",
    "\n",
    "This was about a day long project to help get started and next steps are to make this user friendly and deploy this outside of a notebook environment! Cause let's be honest, these aren't super user friendly.\n",
    "\n",
    "<!-- TEASER_END -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46IEHE9K4du-"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odcOVxDULADn"
   },
   "source": [
    "### Agent Class\n",
    "\n",
    "the main event of all this! each agent is set up to track moves taken over the course of the game and at the end, use `reward` to update the states throughout the game and update the reward function for that game based on the outcome. \n",
    "\n",
    "Using Bellman Equation:\n",
    "\n",
    "\\\\(\\epsilon\\\\): exploration rate - chance that a random move is taken vs best move.  \n",
    "\\\\(\\alpha\\\\): learning rate - basically weights everything and how much impact each reward function has.  \n",
    "\\\\(\\gamma\\\\): discount factor - acting to weight the impact the future reward has.\n",
    "\n",
    "\n",
    "$$Q_{t+1}(s, a) = Q_{t}(s, a) + \\alpha * (\\gamma * reward - Q_t(s, a))$$\n",
    "\n",
    "or in code:\n",
    "\n",
    "```self.states[move] += self.lr * (self.discount_factor * reward - self.states[move])```\n",
    "\n",
    "which can definitely be easier to read :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jdPafLpk5R2t"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "  def __init__(self, symbol, lr=0.2, exp_rate=0.4, discount_factor=0.1):\n",
    "    \"\"\"\n",
    "    initialize a player and associated symbol.\n",
    "    learning rate,\n",
    "    exploration rate,\n",
    "    discount factor\n",
    "    \"\"\"\n",
    "    self.symbol = symbol\n",
    "    self.lr = lr\n",
    "    self.exp_rate = exp_rate\n",
    "    self.discount_factor = discount_factor\n",
    "    self.states = {}\n",
    "    self.moves_taken = []\n",
    "\n",
    "  def _get_values(self, hash):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if hash not in self.states:\n",
    "      values = 0\n",
    "    else:\n",
    "      values = self.states[hash]\n",
    "\n",
    "    return values\n",
    "\n",
    "  def move(self, board):\n",
    "    \"\"\"\n",
    "    choose next move\n",
    "    \"\"\"\n",
    "    explore = np.random.uniform(0, 1) < self.exp_rate\n",
    "    avail_idx = np.argwhere(board.board == 0)\n",
    "    next_move = None\n",
    "    \n",
    "    if explore: # look through all available positions and find best move\n",
    "      max_value = -999\n",
    "      for pos in avail_idx:\n",
    "        next_board = deepcopy(board)\n",
    "        next_board.update_board(pos[0], pos[1], self.symbol)\n",
    "        board_hash = next_board.get_hash()\n",
    "        value = self._get_values(board_hash)\n",
    "\n",
    "        if value > max_value:\n",
    "          max_value = value\n",
    "          next_move = pos\n",
    "    else:\n",
    "      pos_idx = np.random.randint(len(avail_idx))\n",
    "      next_move = avail_idx[pos_idx]\n",
    "\n",
    "    return next_move\n",
    "\n",
    "  def update_move_history(self, board_hash):\n",
    "    \"\"\"\n",
    "    add move to front of the list.\n",
    "    \"\"\"\n",
    "    self.moves_taken.insert(0, board_hash)\n",
    "\n",
    "  def reward(self, reward):\n",
    "    \"\"\"\n",
    "    update rewards at end of game for each state.\n",
    "\n",
    "    Q(S,A)= Q(S,A)+α∗(γ∗maxaQ(S′,a)− Q(S,A))\n",
    "    \"\"\"\n",
    "    # move history is reversed, reward is reward for the next move taken\n",
    "    for move in self.moves_taken:\n",
    "      if move not in self.states:\n",
    "        self.states[move] = 0\n",
    "      self.states[move] += self.lr * (self.discount_factor * reward - self.states[move])\n",
    "      reward = self.states[move]\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    self.moves_taken = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9qkwaUCIh61"
   },
   "source": [
    "### Player class\n",
    "A super basic class to hold the players moves and check for a win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cq85Szs8Ifj8"
   },
   "outputs": [],
   "source": [
    "class Player:\n",
    "  def __init__(self, symbol):\n",
    "    self.symbol = symbol\n",
    "\n",
    "  def make_move(self, row, col, board):\n",
    "    board.update_board(row, col, self.symbol)\n",
    "\n",
    "    board.str_rep()\n",
    "    board.check_win(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pn79f1DTIrCh"
   },
   "source": [
    "### Gameboard\n",
    "You can't play tic-tac-toe without a gameboard! And we definitely want to make sure the board has some basic representation so you don't have to keep track of a numpy array while playing.\n",
    "\n",
    "Some basic functions we want:\n",
    "\n",
    "1. make the board! and track player symbols\n",
    "2. track what moves each players makes and update that symbol appropriately.\n",
    "3. get gameboard hash. the way the agent tracks what moves it has made is by storing a flattened, string representation of the gameboard at that time. we'll have the gameboard class itself take care of this.\n",
    "4. who won? after every move we want to check if anyone won or if there was a tie. the agent won't really learn anything if it never knows what the outcome was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_S7iMfUIdxk"
   },
   "outputs": [],
   "source": [
    "class Gameboard:\n",
    "  def __init__(self, n_rows=3, n_cols=3):\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.board = np.zeros((self.n_rows, self.n_cols))\n",
    "    self.symbols = {0: \" \", 1: \"X\", -1: \"O\"}\n",
    "    self.symbols_rev = {\" \": 0, \"X\": 1, \"O\": -1}\n",
    "    self.winner = None\n",
    "    self.draw = None \n",
    "    self.hash = None\n",
    "\n",
    "  def str_rep(self):\n",
    "    \"\"\"\n",
    "    string representation of array, with symbols\n",
    "    \"\"\"\n",
    "    for i in range(self.n_rows):\n",
    "      row = \"|\"\n",
    "      header = \"|\"\n",
    "      for j in range(self.n_cols):\n",
    "        header += \"---|\"\n",
    "        row += f\" {self.symbols[self.board[i, j]]} |\"\n",
    "      print(header)\n",
    "      print(row)\n",
    "    print(header)\n",
    "\n",
    "  def get_hash(self):\n",
    "    \"\"\"\n",
    "    basically, return flattened view of board\n",
    "    \"\"\"\n",
    "    self.hash = str(self.board.flatten())\n",
    "    return self.hash\n",
    "\n",
    "  def update_board(self, row, col, sym):\n",
    "    \"\"\"\n",
    "    update array with int representation of symbol if position is valid.\n",
    "    \"\"\"\n",
    "    if not isinstance(sym, int):\n",
    "      sym = self.symbols_rev[sym]\n",
    "\n",
    "    if self.board[row, col] == 0:\n",
    "      self.board[row, col] = sym\n",
    "    else:\n",
    "      print(\"move not valid, space not empty\")\n",
    "\n",
    "  def check_win(self, win_score):\n",
    "    \"\"\"\n",
    "    check diagonals, rows, and columns for win_score (+ or -) or draw.\n",
    "    \"\"\"\n",
    "    left_diag_score = np.sum(np.diag(self.board))\n",
    "    right_diag_score = np.sum(np.diag(np.fliplr(self.board)))\n",
    "\n",
    "    row_sums = np.sum(self.board, axis=1)\n",
    "    col_sums = np.sum(self.board, axis=0)\n",
    "\n",
    "    x_win =  win_score\n",
    "    o_win = win_score * -1\n",
    "\n",
    "    # check if x wins\n",
    "    if x_win in row_sums or x_win in col_sums or x_win in [left_diag_score, right_diag_score]:\n",
    "      self.winner = 1\n",
    "      print(\"game over, X wins\")\n",
    "      return True\n",
    "\n",
    "    # check if o wins\n",
    "    if o_win in row_sums or o_win in col_sums or o_win in [left_diag_score, right_diag_score]:\n",
    "      self.winner = -1\n",
    "      print(\"game over, O wins\")\n",
    "      return True\n",
    "\n",
    "    # check if draw\n",
    "    if 0 not in self.board:\n",
    "      self.draw = True\n",
    "      self.winner = 0\n",
    "      print(\"game over, draw\")\n",
    "      return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "    back to square one\n",
    "    \"\"\"\n",
    "    self.board = np.zeros((self.n_rows, self.n_cols))\n",
    "    self.winner = None\n",
    "    self.draw = None \n",
    "    self.hash = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7NONKJviKGm6"
   },
   "source": [
    "### Helper functions\n",
    "\n",
    "A couple functions to help with training as well as having the agent make a move when a person is playing against it.\n",
    "\n",
    "To train, it's basically just set up to have two agents play against eachother for `x` rounds, and to appropriately check, update, and award each agent.\n",
    "\n",
    "after this, it's time to play :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDjGrQmgEc6X"
   },
   "outputs": [],
   "source": [
    "def train_agent(player1, player2, rounds):\n",
    "  player1_wins = np.zeros(rounds)\n",
    "  player2_wins = np.zeros(rounds)\n",
    "\n",
    "  win_score = 3\n",
    "\n",
    "  for i in range(rounds):\n",
    "    board = Gameboard()\n",
    "    symbol_map = board.symbols\n",
    "    print(f\"starting round {i+1}\")\n",
    "    while board.winner is None:\n",
    "      for player in [player1, player2]:\n",
    "        move = player.move(board)\n",
    "        board.update_board(move[0], move[1], symbol_map[player.symbol])\n",
    "        player.update_move_history(board.get_hash())\n",
    "        winner = board.check_win(win_score)\n",
    "\n",
    "        # in case player one fills last spot\n",
    "        if board.winner is not None:\n",
    "          break\n",
    "\n",
    "    if winner and board.winner == player1.symbol:\n",
    "      player1.reward(1)\n",
    "      player2.reward(-1)\n",
    "      player1_wins[i] = 1\n",
    "    elif winner and board.winner == player2.symbol:\n",
    "      player1.reward(-1)\n",
    "      player2.reward(1)\n",
    "      player2_wins[i] = 1\n",
    "    elif not winner and board.winner == 0:\n",
    "      player1.reward(-1)\n",
    "      player2.reward(-1)\n",
    "\n",
    "    player1.reset()\n",
    "    player2.reset()\n",
    "\n",
    "  return player1_wins, player2_wins\n",
    "\n",
    "def agent_move(player, board):\n",
    "  if board.winner is None:\n",
    "    move = player.move(board)\n",
    "    board.update_board(move[0], move[1], player.symbol)\n",
    "    player.update_move_history(board.get_hash())\n",
    "\n",
    "  board.str_rep()\n",
    "  win = board.check_win(3)\n",
    "\n",
    "  if win and board.winner == player.symbol:\n",
    "    player.reward(1)\n",
    "    player.reset()\n",
    "  else:\n",
    "    player.reward(-1)\n",
    "    player.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uw89j3m5EdSG"
   },
   "source": [
    "## Let's Play!\n",
    "\n",
    "### Training\n",
    "Starting out by training two agents to play against each other. For initial training going for 100 rounds before playing with first agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rTcKcU0f0K5R"
   },
   "outputs": [],
   "source": [
    "player1 = Agent(symbol=1)\n",
    "player2 = Agent(symbol=-1)\n",
    "\n",
    "p1_wins, p2_wins = train_agent(player1, player2, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x6Kt_a1--PWe",
    "outputId": "4ffb7258-2a9d-43e8-bbe6-f19853f9cd64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3148.0, 1375.0)"
      ]
     },
     "execution_count": 350,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(p1_wins), np.sum(p2_wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3vWhmlaV-mhy",
    "outputId": "ca24a719-693d-400e-ead3-c0fbdc16795f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6959982312624364"
      ]
     },
     "execution_count": 351,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(p1_wins) / (np.sum(p1_wins) + np.sum(p2_wins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCgj1n6YFajv"
   },
   "source": [
    "### Me vs Agent\n",
    "currently pretty manual process. planning to automate this and set something up for player to actually play against the agent and more than just a string represented gameboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GsUct5K6eMS"
   },
   "outputs": [],
   "source": [
    "me = Player(symbol=-1)\n",
    "game = Gameboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "4Db8yd4s6nCb",
    "outputId": "f9f1ccec-bc4b-4a29-b031-a55eb8382a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n"
     ]
    }
   ],
   "source": [
    "agent_move(player1, game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "juC4LPWS9LFT",
    "outputId": "c1ff8c71-7f0c-4507-c4c4-4cbe6bee25bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O |   |   |\n",
      "|---|---|---|\n",
      "| X |   |   |\n",
      "|---|---|---|\n"
     ]
    }
   ],
   "source": [
    "me.make_move(1, 0, game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "C0KrbXVz7Tlt",
    "outputId": "6c703c14-e215-4368-fab0-1e12b5f036c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O |   |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n"
     ]
    }
   ],
   "source": [
    "agent_move(player1, game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "6YmhD95x7o0I",
    "outputId": "0763d0ba-21ec-47a0-baa4-a464a0873e28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "|---|---|---|\n",
      "| X |   | X |\n",
      "|---|---|---|\n"
     ]
    }
   ],
   "source": [
    "me.make_move(1, 1, game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "cQ38SU_E7uRy",
    "outputId": "e5858699-be81-45a1-ec05-e23972c62e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|---|---|---|\n",
      "|   |   |   |\n",
      "|---|---|---|\n",
      "| O | O |   |\n",
      "|---|---|---|\n",
      "| X | X | X |\n",
      "|---|---|---|\n",
      "game over, X wins\n"
     ]
    }
   ],
   "source": [
    "agent_move(player1, game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqSSplFx5E-A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nikola": {
   "category": "",
   "date": "2020-03-21 15:59:57 UTC-07:00",
   "description": "",
   "link": "",
   "slug": "playing-with-tic-tac-toe",
   "tags": "",
   "title": "Playing with Tic Tac Toe",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}